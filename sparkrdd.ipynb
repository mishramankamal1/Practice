{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.242.134:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1623732855827)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@15282a66\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.\n",
      "\n",
      "\n",
      "101,aaaa,40000,m,11\n",
      "\n",
      "\n",
      "102,bbbbbb,50000,f,12\n",
      "\n",
      "\n",
      "103,cccc,50000,m,12\n",
      "\n",
      "\n",
      "104,dd,90000,f,13\n",
      "\n",
      "\n",
      "105,ee,10000,m,12\n",
      "\n",
      "\n",
      "106,dkd,40000,m,12\n",
      "\n",
      "\n",
      "107,sdkfj,80000,f,13\n",
      "\n",
      "\n",
      "108,iiii,50000,m,11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[String] = /data/* MapPartitionsRDD[4] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.textFile(\"/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101,aaaa,40000,m,11\n",
      "102,bbbbbb,50000,f,12\n",
      "103,cccc,50000,m,12\n",
      "104,dd,90000,f,13\n",
      "105,ee,10000,m,12\n",
      "106,dkd,40000,m,12\n",
      "107,sdkfj,80000,f,13\n",
      "108,iiii,50000,m,11\n"
     ]
    }
   ],
   "source": [
    "data.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[5] at map at <console>:26\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = data.map{x=>\n",
    "    val w = x.split(\",\")\n",
    "    val dept = w(4)\n",
    "    val gender = w(3)\n",
    "    val salary = w(2).toInt\n",
    "    ((dept,gender),salary)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((11,m),40000)\n",
      "((12,f),50000)\n",
      "((12,m),50000)\n",
      "((13,f),90000)\n",
      "((12,m),10000)\n",
      "((12,m),40000)\n",
      "((13,f),80000)\n",
      "((11,m),50000)\n"
     ]
    }
   ],
   "source": [
    "rdd1.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Select dept, gender,sum(salary) from emp group by dept,gender;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[6] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = rdd1.reduceByKey((a,b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((12,f),50000)\n",
      "((13,f),170000)\n",
      "((12,m),100000)\n",
      "((11,m),90000)\n"
     ]
    }
   ],
   "source": [
    "rdd2.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop is the Elephant King\n",
      "A yellow and elegant thing\n",
      "He never forgets\n",
      "Useful data or lets\n",
      "An extraneous element cling\n",
      "A wonderful king is Hadoop\n",
      "The elephant plays well with Sqoop\n",
      "But what helps him to thrive\n",
      "Are Impala and Hive\n",
      "And HDFS in the group\n",
      "Hadoop is an elegant fellow\n",
      "An elephant gentle and mellow\n",
      "He never gets mad\n",
      "Or does anything bad\n",
      "Because at his core he is yellow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wdata: org.apache.spark.rdd.RDD[String] = /data/wc1.txt MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wdata = sc.textFile(\"/data/wc1.txt\")\n",
    "wdata.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop\n",
      "is\n",
      "the\n",
      "Elephant\n",
      "King\n",
      "A\n",
      "yellow\n",
      "and\n",
      "elegant\n",
      "thing\n",
      "He\n",
      "never\n",
      "forgets\n",
      "Useful\n",
      "data\n",
      "or\n",
      "lets\n",
      "An\n",
      "extraneous\n",
      "element\n",
      "cling\n",
      "A\n",
      "wonderful\n",
      "king\n",
      "is\n",
      "Hadoop\n",
      "The\n",
      "elephant\n",
      "plays\n",
      "well\n",
      "with\n",
      "Sqoop\n",
      "But\n",
      "what\n",
      "helps\n",
      "him\n",
      "to\n",
      "thrive\n",
      "Are\n",
      "Impala\n",
      "and\n",
      "Hive\n",
      "And\n",
      "HDFS\n",
      "in\n",
      "the\n",
      "group\n",
      "Hadoop\n",
      "is\n",
      "an\n",
      "elegant\n",
      "fellow\n",
      "An\n",
      "elephant\n",
      "gentle\n",
      "and\n",
      "mellow\n",
      "He\n",
      "never\n",
      "gets\n",
      "mad\n",
      "Or\n",
      "does\n",
      "anything\n",
      "bad\n",
      "Because\n",
      "at\n",
      "his\n",
      "core\n",
      "he\n",
      "is\n",
      "yellow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flattenData: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flattenData = wdata.flatMap(x=>x.split(\" \"))\n",
    "flattenData.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hadoop,1)\n",
      "(is,1)\n",
      "(the,1)\n",
      "(elephant,1)\n",
      "(king,1)\n",
      "(a,1)\n",
      "(yellow,1)\n",
      "(and,1)\n",
      "(elegant,1)\n",
      "(thing,1)\n",
      "(he,1)\n",
      "(never,1)\n",
      "(forgets,1)\n",
      "(useful,1)\n",
      "(data,1)\n",
      "(or,1)\n",
      "(lets,1)\n",
      "(an,1)\n",
      "(extraneous,1)\n",
      "(element,1)\n",
      "(cling,1)\n",
      "(a,1)\n",
      "(wonderful,1)\n",
      "(king,1)\n",
      "(is,1)\n",
      "(hadoop,1)\n",
      "(the,1)\n",
      "(elephant,1)\n",
      "(plays,1)\n",
      "(well,1)\n",
      "(with,1)\n",
      "(sqoop,1)\n",
      "(but,1)\n",
      "(what,1)\n",
      "(helps,1)\n",
      "(him,1)\n",
      "(to,1)\n",
      "(thrive,1)\n",
      "(are,1)\n",
      "(impala,1)\n",
      "(and,1)\n",
      "(hive,1)\n",
      "(and,1)\n",
      "(hdfs,1)\n",
      "(in,1)\n",
      "(the,1)\n",
      "(group,1)\n",
      "(hadoop,1)\n",
      "(is,1)\n",
      "(an,1)\n",
      "(elegant,1)\n",
      "(fellow,1)\n",
      "(an,1)\n",
      "(elephant,1)\n",
      "(gentle,1)\n",
      "(and,1)\n",
      "(mellow,1)\n",
      "(he,1)\n",
      "(never,1)\n",
      "(gets,1)\n",
      "(mad,1)\n",
      "(or,1)\n",
      "(does,1)\n",
      "(anything,1)\n",
      "(bad,1)\n",
      "(because,1)\n",
      "(at,1)\n",
      "(his,1)\n",
      "(core,1)\n",
      "(he,1)\n",
      "(is,1)\n",
      "(yellow,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keyPair: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at <console>:28\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val keyPair = flattenData.map(_.toLowerCase).map(word => (word,1))\n",
    "keyPair.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "28: error: value sortByValue is not a member of org.apache.spark.rdd.RDD[(String, Int)]",
     "output_type": "error",
     "traceback": [
      "<console>:28: error: value sortByValue is not a member of org.apache.spark.rdd.RDD[(String, Int)]",
      "       val getTheCount = keyPair.reduceByKey((a,b) => a + b).sortByValue(true,1)",
      "                                                             ^",
      ""
     ]
    }
   ],
   "source": [
    "val getTheCount = keyPair.reduceByKey((a,b) => a + b).sortByValue(true,1)\n",
    "getTheCount.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
